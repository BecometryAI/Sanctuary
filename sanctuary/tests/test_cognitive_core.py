"""
Unit tests for cognitive_core placeholder classes.

Tests cover:
- Proper initialization of all classes
- Import structure and module organization
- Type hints and docstring presence
- Data model validation
- PEP 8 compliance
- Integration tests for CognitiveCore
"""

import gc
import pytest
import pytest_asyncio
import asyncio
import shutil
import tempfile
import time
from pathlib import Path

from mind.cognitive_core.core import CognitiveCore
from mind.cognitive_core.workspace import (
    GlobalWorkspace, WorkspaceContent, Percept as WorkspacePercept,
    Goal, GoalType
)
from mind.cognitive_core.attention import AttentionController, AttentionMode, AttentionScore
from mind.cognitive_core.perception import PerceptionSubsystem, ModalityType, Percept
from mind.cognitive_core.action import ActionSubsystem, ActionType, Action
from mind.cognitive_core.affect import AffectSubsystem, EmotionalState
from mind.cognitive_core.meta_cognition import SelfMonitor


@pytest.fixture
def temp_dirs():
    """Create temporary directories for ChromaDB to avoid schema conflicts."""
    temp_base = tempfile.mkdtemp()
    data_dir = Path(temp_base) / "data"
    chroma_dir = Path(temp_base) / "chroma"
    identity_dir = Path(temp_base) / "identity"
    data_dir.mkdir(parents=True, exist_ok=True)
    chroma_dir.mkdir(parents=True, exist_ok=True)
    identity_dir.mkdir(parents=True, exist_ok=True)

    # Create identity files so IdentityLoader doesn't hit fallback path
    (identity_dir / "charter.md").write_text(
        "# Core Values\n- Truthfulness\n- Helpfulness\n- Harmlessness\n\n"
        "# Purpose Statement\nTo think, learn, and interact authentically.\n\n"
        "# Behavioral Guidelines\n- Be honest\n- Be helpful\n- Be thoughtful\n"
    )
    (identity_dir / "protocols.md").write_text(
        "```yaml\n- name: Uncertainty Acknowledgment\n"
        "  description: When uncertain, acknowledge it\n"
        "  trigger_conditions:\n    - Low confidence in response\n"
        "  actions:\n    - Express uncertainty explicitly\n"
        "  priority: 0.8\n```\n"
    )

    yield {"base_dir": str(data_dir), "chroma_dir": str(chroma_dir), "identity_dir": str(identity_dir)}

    # Cleanup with retry for Windows file locking
    gc.collect()
    for attempt in range(3):
        try:
            shutil.rmtree(temp_base)
            break
        except PermissionError:
            if attempt < 2:
                time.sleep(0.5)
                gc.collect()


def make_core_config(temp_dirs):
    """Create config with temp directories for CognitiveCore."""
    return {
        "identity_dir": temp_dirs["identity_dir"],
        "perception": {"mock_mode": True, "mock_embedding_dim": 384},
        "memory": {
            "memory_config": {
                "base_dir": temp_dirs["base_dir"],
                "chroma_dir": temp_dirs["chroma_dir"],
            }
        }
    }


class TestCognitiveCore:
    """Test CognitiveCore class initialization and structure"""

    def test_cognitive_core_initialization_default(self, temp_dirs):
        """Test creating CognitiveCore with default parameters"""
        config = make_core_config(temp_dirs)
        core = CognitiveCore(config=config)
        assert core is not None
        assert isinstance(core, CognitiveCore)
        assert core.workspace is not None
        assert core.attention is not None
        assert core.perception is not None
        assert core.action is not None
        assert core.affect is not None
        assert core.meta_cognition is not None
        assert core.running is False

    def test_cognitive_core_initialization_custom(self, temp_dirs):
        """Test creating CognitiveCore with custom parameters"""
        workspace = GlobalWorkspace(capacity=10)
        config = make_core_config(temp_dirs)
        config["cycle_rate_hz"] = 20
        config["attention_budget"] = 150
        core = CognitiveCore(workspace=workspace, config=config)
        assert core is not None
        assert isinstance(core, CognitiveCore)
        assert core.workspace == workspace
        assert core.config["cycle_rate_hz"] == 20
        assert core.config["attention_budget"] == 150
    
    def test_cognitive_core_has_docstring(self):
        """Test that CognitiveCore has comprehensive docstring"""
        assert CognitiveCore.__doc__ is not None
        assert len(CognitiveCore.__doc__) > 100
        assert "recurrent" in CognitiveCore.__doc__.lower()
    
    def test_cognitive_core_init_signature(self):
        """Test that __init__ has proper type hints"""
        import inspect
        sig = inspect.signature(CognitiveCore.__init__)
        assert 'workspace' in sig.parameters
        assert 'config' in sig.parameters


class TestCognitiveCoreSingleCycle:
    """Test single cognitive cycle execution"""

    @pytest.mark.asyncio
    async def test_single_cycle_via_executor(self, temp_dirs):
        """Test running one cognitive cycle via the cycle executor"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Execute a single cycle directly through the executor
        await core.cycle_executor.execute_cycle()

        # Verify cycle executed (workspace cycle count increments)
        assert workspace.cycle_count >= 1

    @pytest.mark.asyncio
    async def test_workspace_updated_after_cycle(self, temp_dirs):
        """Test that workspace is updated after a cycle"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        initial_cycle_count = workspace.cycle_count

        # Execute a single cycle directly through the executor
        await core.cycle_executor.execute_cycle()

        # Verify workspace was updated
        assert workspace.cycle_count > initial_cycle_count


class TestCognitiveCoreInputInjection:
    """Test input injection functionality"""

    @pytest.mark.asyncio
    async def test_inject_input_after_start(self, temp_dirs):
        """Test injecting raw input via inject_input() after starting"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core to initialize queues
        task = asyncio.create_task(core.start())
        await asyncio.sleep(0.1)  # Let it initialize

        # Inject raw text input
        core.inject_input("test input", modality="text")

        # Verify it's in the queue
        assert not core.input_queue.empty()

        # Verify it's a tuple of (raw_input, modality)
        item = core.input_queue.get_nowait()
        assert item == ("test input", "text")

        # Stop the core
        await core.stop()

    @pytest.mark.asyncio
    async def test_injected_percept_appears_in_workspace(self, temp_dirs):
        """Test that injected input is encoded and appears in workspace after cycle"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core
        task = asyncio.create_task(core.start())
        await asyncio.sleep(0.1)

        # Inject raw text input (will be encoded by perception subsystem)
        core.inject_input("test input", modality="text")

        # Let cycles run
        await asyncio.sleep(0.3)

        # Verify cycles ran (the input was processed in some way)
        assert core.metrics['total_cycles'] >= 1

        # Stop the core
        await core.stop()

    @pytest.mark.asyncio
    async def test_inject_input_before_start_auto_initializes(self, temp_dirs):
        """Test that injecting input before start auto-initializes queues."""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Inject input before starting â€” should auto-initialize queues
        core.inject_input("test input", modality="text")
        assert core.state.input_queue is not None
        assert not core.state.input_queue.empty()


class TestCognitiveCoreAttentionIntegration:
    """Test attention integration"""

    @pytest.mark.asyncio
    async def test_attention_selects_highest_priority(self, temp_dirs):
        """Test that attention selects highest priority percepts"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        config["attention_budget"] = 50
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core to initialize queues
        task = asyncio.create_task(core.start())
        await asyncio.sleep(0.1)

        # Inject multiple raw text inputs
        for i in range(5):
            core.inject_input(f"test input {i}", modality="text")

        # Let cycles run
        await asyncio.sleep(0.3)

        # Stop the core
        await core.stop()

        # Verify attention made selections
        assert core.metrics['attention_selections'] >= 0
        # Verify cycles ran
        assert core.metrics['total_cycles'] > 0


class TestCognitiveCoreCycleRate:
    """Test cycle rate management"""

    @pytest.mark.asyncio
    async def test_cycle_rate_timing(self, temp_dirs):
        """Test that cycle rate is approximately correct"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        config["cycle_rate_hz"] = 10  # 100ms per cycle
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core
        task = asyncio.create_task(core.start())

        # Run for ~500ms to get ~5 cycles
        await asyncio.sleep(0.5)

        # Stop the core
        await core.stop()

        # Should have run approximately 5 cycles (100ms each)
        # Allow some tolerance
        assert 3 <= core.metrics['total_cycles'] <= 8

    @pytest.mark.asyncio
    async def test_average_cycle_time(self, temp_dirs):
        """Test average cycle time tracking"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core and let it run
        task = asyncio.create_task(core.start())
        await asyncio.sleep(0.5)
        await core.stop()

        # Verify metrics
        metrics = core.get_metrics()
        assert metrics['total_cycles'] >= 1
        assert 'avg_cycle_time_ms' in metrics
        assert metrics['avg_cycle_time_ms'] > 0


class TestCognitiveCoreGracefulShutdown:
    """Test graceful shutdown"""

    @pytest.mark.asyncio
    async def test_graceful_shutdown(self, temp_dirs):
        """Test that stop() gracefully shuts down the core"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Start core in background
        async def run_core():
            await core.start()

        task = asyncio.create_task(run_core())

        # Let it run a few cycles
        await asyncio.sleep(0.3)

        # Stop it
        await core.stop()

        # Wait for task to complete
        await asyncio.sleep(0.1)

        # Verify it stopped
        assert core.running is False
        assert core.metrics['total_cycles'] > 0


class TestCognitiveCoreErrorRecovery:
    """Test error recovery"""

    @pytest.mark.asyncio
    async def test_loop_continues_despite_error(self, temp_dirs):
        """Test that loop continues despite errors"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core
        task = asyncio.create_task(core.start())
        await asyncio.sleep(0.1)

        # Inject a malformed percept (this should be handled gracefully)
        # Note: Since we're using Pydantic models, malformed percepts
        # can't really be created, but we can test with extreme values
        percept = WorkspacePercept(
            modality="text",
            raw="",  # Empty raw content
            embedding=[],  # Empty embedding
            complexity=0
        )
        core.inject_input(percept)

        # Let the loop continue
        await asyncio.sleep(0.3)

        # Stop the core
        await core.stop()

        # Verify loop continued
        assert core.metrics['total_cycles'] >= 2


class TestCognitiveCoreStateQuery:
    """Test state query functionality"""

    @pytest.mark.asyncio
    async def test_query_state(self, temp_dirs):
        """Test querying current state"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Add a goal to workspace
        goal = Goal(
            type=GoalType.RESPOND_TO_USER,
            description="Test goal"
        )
        workspace.add_goal(goal)

        # Query state
        snapshot = core.query_state()

        # Verify snapshot
        assert snapshot is not None
        assert len(snapshot.goals) == 1
        assert snapshot.goals[0].description == "Test goal"

    @pytest.mark.asyncio
    async def test_query_state_is_immutable(self, temp_dirs):
        """Test that queried state is immutable"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Query state
        snapshot = core.query_state()

        # Attempt to modify should fail (Pydantic frozen model)
        with pytest.raises(Exception):  # ValidationError or AttributeError
            snapshot.cycle_count = 999


class TestCognitiveCoreMetrics:
    """Test metrics functionality"""

    @pytest.mark.asyncio
    async def test_get_metrics(self, temp_dirs):
        """Test getting performance metrics"""
        workspace = GlobalWorkspace()
        config = make_core_config(temp_dirs)
        core = CognitiveCore(workspace=workspace, config=config)

        # Start the core and run a few cycles
        task = asyncio.create_task(core.start())
        await asyncio.sleep(0.5)
        await core.stop()

        # Get metrics
        metrics = core.get_metrics()

        # Verify metrics structure
        assert 'total_cycles' in metrics
        assert 'avg_cycle_time_ms' in metrics
        assert 'target_cycle_time_ms' in metrics
        assert 'cycle_rate_hz' in metrics
        assert 'attention_selections' in metrics
        assert 'percepts_processed' in metrics
        assert 'workspace_size' in metrics
        assert 'current_goals' in metrics

        # Verify values
        assert metrics['total_cycles'] >= 1
        assert metrics['cycle_rate_hz'] == 10  # default


class TestGlobalWorkspace:
    """Test GlobalWorkspace class initialization and data models"""
    
    def test_workspace_initialization_default(self):
        """Test creating GlobalWorkspace with default parameters"""
        workspace = GlobalWorkspace()
        assert workspace is not None
        assert isinstance(workspace, GlobalWorkspace)
    
    def test_workspace_initialization_custom(self):
        """Test creating GlobalWorkspace with custom parameters"""
        workspace = GlobalWorkspace(capacity=10, persistence_dir="/tmp/test")
        assert workspace is not None
    
    def test_workspace_has_docstring(self):
        """Test that GlobalWorkspace has comprehensive docstring"""
        assert GlobalWorkspace.__doc__ is not None
        assert len(GlobalWorkspace.__doc__) > 100
        assert "conscious" in GlobalWorkspace.__doc__.lower()
        assert "broadcast" in GlobalWorkspace.__doc__.lower()
    
    def test_workspace_content_creation(self):
        """Test creating WorkspaceContent data class"""
        content = WorkspaceContent(
            goals=["test goal"],
            percepts=[{"type": "text", "content": "test"}],
            emotions={"valence": 0.5}
        )
        assert content is not None
        assert len(content.goals) == 1
        assert len(content.percepts) == 1
        assert "valence" in content.emotions
    
    def test_workspace_content_defaults(self):
        """Test WorkspaceContent default values"""
        content = WorkspaceContent()
        assert isinstance(content.goals, list)
        assert isinstance(content.percepts, list)
        assert isinstance(content.emotions, dict)
        assert isinstance(content.memories, list)
        assert isinstance(content.metadata, dict)


class TestAttentionController:
    """Test AttentionController class initialization and enums"""
    
    def test_attention_initialization_default(self):
        """Test creating AttentionController with default parameters"""
        attention = AttentionController()
        assert attention is not None
        assert isinstance(attention, AttentionController)
    
    def test_attention_initialization_custom(self):
        """Test creating AttentionController with custom parameters"""
        attention = AttentionController(
            initial_mode=AttentionMode.DIFFUSE,
            goal_weight=0.5,
            novelty_weight=0.3
        )
        assert attention is not None
    
    def test_attention_mode_enum(self):
        """Test AttentionMode enum values"""
        assert AttentionMode.FOCUSED.value == "focused"
        assert AttentionMode.DIFFUSE.value == "diffuse"
        assert AttentionMode.VIGILANT.value == "vigilant"
        assert AttentionMode.RELAXED.value == "relaxed"
    
    def test_attention_score_creation(self):
        """Test creating AttentionScore data class"""
        score = AttentionScore(
            goal_relevance=0.8,
            novelty=0.6,
            emotional_salience=0.7,
            urgency=0.5,
            total=0.65
        )
        assert score is not None
        assert score.goal_relevance == 0.8
        assert score.total == 0.65
    
    def test_attention_has_docstring(self):
        """Test that AttentionController has comprehensive docstring"""
        assert AttentionController.__doc__ is not None
        assert len(AttentionController.__doc__) > 100
        assert "attention" in AttentionController.__doc__.lower()


class TestPerceptionSubsystem:
    """Test PerceptionSubsystem class initialization and data models"""
    
    def test_perception_initialization_default(self):
        """Test creating PerceptionSubsystem with default parameters"""
        perception = PerceptionSubsystem()
        assert perception is not None
        assert isinstance(perception, PerceptionSubsystem)
    
    def test_perception_initialization_custom(self):
        """Test creating PerceptionSubsystem with custom parameters"""
        config = {
            "text_model": "all-MiniLM-L6-v2",
            "cache_size": 500,
            "enable_image": False,
        }
        perception = PerceptionSubsystem(config=config)
        assert perception is not None
        assert perception.cache_size == 500
    
    def test_modality_type_enum(self):
        """Test ModalityType enum values"""
        assert ModalityType.TEXT.value == "text"
        assert ModalityType.IMAGE.value == "image"
        assert ModalityType.AUDIO.value == "audio"
        assert ModalityType.PROPRIOCEPTIVE.value == "proprioceptive"
    
    def test_percept_creation(self):
        """Test creating Percept data class"""
        import numpy as np
        embedding = np.array([0.1, 0.2, 0.3], dtype=np.float32)
        percept = Percept(
            embedding=embedding,
            modality=ModalityType.TEXT,
            confidence=0.95
        )
        assert percept is not None
        assert percept.modality == ModalityType.TEXT
        assert percept.confidence == 0.95
        assert percept.metadata is not None
    
    def test_perception_has_docstring(self):
        """Test that PerceptionSubsystem has comprehensive docstring"""
        assert PerceptionSubsystem.__doc__ is not None
        assert len(PerceptionSubsystem.__doc__) > 100
        assert "perception" in PerceptionSubsystem.__doc__.lower()


class TestActionSubsystem:
    """Test ActionSubsystem class initialization and data models"""
    
    def test_action_initialization_default(self):
        """Test creating ActionSubsystem with default parameters"""
        action_sys = ActionSubsystem()
        assert action_sys is not None
        assert isinstance(action_sys, ActionSubsystem)
    
    def test_action_initialization_custom(self):
        """Test creating ActionSubsystem with custom parameters"""
        config = {"test_key": "test_value"}
        action_sys = ActionSubsystem(config=config)
        assert action_sys is not None
        assert action_sys.config["test_key"] == "test_value"
    
    def test_action_type_enum(self):
        """Test ActionType enum values"""
        assert ActionType.SPEAK.value == "speak"
        assert ActionType.COMMIT_MEMORY.value == "commit_memory"
        assert ActionType.RETRIEVE_MEMORY.value == "retrieve_memory"
        assert ActionType.INTROSPECT.value == "introspect"
        assert ActionType.UPDATE_GOAL.value == "update_goal"
        assert ActionType.WAIT.value == "wait"
        assert ActionType.TOOL_CALL.value == "tool_call"
    
    def test_action_creation(self):
        """Test creating Action Pydantic model"""
        action = Action(
            type=ActionType.SPEAK,
            parameters={"text": "Hello"},
            priority=0.8
        )
        assert action is not None
        assert action.type == ActionType.SPEAK
        assert action.priority == 0.8
        assert action.metadata is not None
    
    def test_action_has_docstring(self):
        """Test that ActionSubsystem has comprehensive docstring"""
        assert ActionSubsystem.__doc__ is not None
        assert len(ActionSubsystem.__doc__) > 100
        assert "action" in ActionSubsystem.__doc__.lower()


class TestAffectSubsystem:
    """Test AffectSubsystem class initialization and data models"""
    
    def test_affect_initialization_default(self):
        """Test creating AffectSubsystem with default parameters"""
        affect = AffectSubsystem()
        assert affect is not None
        assert isinstance(affect, AffectSubsystem)
    
    def test_affect_initialization_custom(self):
        """Test creating AffectSubsystem with custom parameters"""
        config = {
            "baseline": {
                "valence": 0.1,
                "arousal": -0.2,
                "dominance": 0.5
            },
            "decay_rate": 0.15
        }
        affect = AffectSubsystem(config=config)
        assert affect is not None
        assert affect.baseline["valence"] == 0.1
        assert affect.baseline["arousal"] == -0.2
        assert affect.decay_rate == 0.15
    
    def test_emotional_state_creation(self):
        """Test creating EmotionalState data class"""
        state = EmotionalState(
            valence=0.5,
            arousal=0.3,
            dominance=0.2
        )
        assert state is not None
        assert state.valence == 0.5
        assert state.arousal == 0.3
        assert state.dominance == 0.2
        assert state.intensity > 0.0  # Calculated in __post_init__
        assert state.labels is not None
    
    def test_emotional_state_to_vector(self):
        """Test converting EmotionalState to numpy vector"""
        import numpy as np
        state = EmotionalState(valence=0.5, arousal=0.3, dominance=0.2)
        vector = state.to_vector()
        assert isinstance(vector, np.ndarray)
        assert len(vector) == 3
        assert vector[0] == 0.5
    
    def test_affect_has_docstring(self):
        """Test that AffectSubsystem has comprehensive docstring"""
        assert AffectSubsystem.__doc__ is not None
        assert len(AffectSubsystem.__doc__) > 100
        assert "emotion" in AffectSubsystem.__doc__.lower()


class TestSelfMonitor:
    """Test SelfMonitor class initialization and data models"""

    def test_monitor_initialization_default(self):
        """Test creating SelfMonitor with default parameters"""
        monitor = SelfMonitor()
        assert monitor is not None
        assert isinstance(monitor, SelfMonitor)

    def test_monitor_initialization_with_workspace(self):
        """Test creating SelfMonitor with workspace"""
        workspace = GlobalWorkspace()
        monitor = SelfMonitor(workspace=workspace)
        assert monitor is not None
        assert monitor.workspace == workspace

    def test_monitor_initialization_with_config(self):
        """Test creating SelfMonitor with custom config"""
        config = {
            "monitoring_frequency": 5,
            "enable_existential_questions": False,
            "enable_capability_tracking": True
        }
        monitor = SelfMonitor(config=config)
        assert monitor is not None
        assert monitor.monitoring_frequency == 5
        assert monitor.enable_existential_questions is False
        assert monitor.enable_capability_tracking is True

    def test_monitor_has_stats(self):
        """Test that SelfMonitor initializes stats properly"""
        monitor = SelfMonitor()
        assert "total_observations" in monitor.stats
        assert "value_conflicts" in monitor.stats
        assert "predictions_made" in monitor.stats

    def test_monitor_has_self_model(self):
        """Test that SelfMonitor initializes self-model properly"""
        monitor = SelfMonitor()
        assert "capabilities" in monitor.self_model
        assert "limitations" in monitor.self_model
        assert "preferences" in monitor.self_model
        assert "values_hierarchy" in monitor.self_model

    def test_monitor_has_docstring(self):
        """Test that SelfMonitor has comprehensive docstring"""
        assert SelfMonitor.__doc__ is not None
        assert len(SelfMonitor.__doc__) > 100
        assert "meta" in SelfMonitor.__doc__.lower() or "introspect" in SelfMonitor.__doc__.lower()


class TestModuleStructure:
    """Test module-level structure and imports"""
    
    def test_cognitive_core_module_docstring(self):
        """Test cognitive_core module has proper docstring"""
        import mind.cognitive_core as cc
        assert cc.__doc__ is not None
        assert "Cognitive Core" in cc.__doc__
        assert "non-linguistic" in cc.__doc__.lower()  # Case-insensitive check
    
    def test_cognitive_core_exports(self):
        """Test cognitive_core __all__ exports"""
        import mind.cognitive_core as cc
        assert hasattr(cc, '__all__')
        assert 'CognitiveCore' in cc.__all__
        assert 'GlobalWorkspace' in cc.__all__
        assert 'AttentionController' in cc.__all__
        assert 'PerceptionSubsystem' in cc.__all__
        assert 'ActionSubsystem' in cc.__all__
        assert 'AffectSubsystem' in cc.__all__
        assert 'SelfMonitor' in cc.__all__
    
    def test_interfaces_module_docstring(self):
        """Test interfaces module has proper docstring"""
        import mind.interfaces as li
        assert li.__doc__ is not None
        assert "Language Interfaces" in li.__doc__
        assert "peripheral" in li.__doc__.lower()
    
    def test_interfaces_exports(self):
        """Test interfaces __all__ exports"""
        import mind.interfaces as li
        assert hasattr(li, '__all__')
        assert 'LanguageInputParser' in li.__all__
        assert 'LanguageOutputGenerator' in li.__all__
